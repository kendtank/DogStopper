## tinyml mfcc 模型的设计到量化转换为字节cc数组移植到mcu的闭环流程

## 注意：需要好好按照步骤进行，这样得到的模型才是最终的正确的模型和在TensorflowLite中运行的一致，F1误差不超过0.01, 模型推理结果误差不超过5%


### tinyml 模型训练到移植mcu的方法论
1. 收集狗吠数据集音频数据，进行裁剪，尽可能只包含狗吠
2. 对数据进行预处理，保证音频格式为wav和采样率16k， 单通道音频(本项目没有提供脚本清洗)
3. 对收集到了音频数据进行数据集的划分，训练集和测试集， 格式如下：主目录下有no_bark和dog_bark两个目录，每个目录下都是处理好的音频文件/*.wav
4. 在tinyml目录下 cd进入dataset目录，执行 python split_dataset.py 得到训练集和测试集txt文件 (包含了绝对路径和对应的标签)
5. 得到了train.txt和test.txt两个文件，然后进入dataset目录，执行 python compute_robust_norm_params.py 获取归一化的参数(自定义的mfcc特征值范围不在INT8的范围内，所以需要归一化,也有利于模型的收敛)
6. 实现的norm_mfcc.py, 就是在py端调用norm_mfcc.npy文件的参数对mfcc进行归一化， mcu端也需要拿到归一化的参数（norm_mfcc.h文件）实现在mcu端的归一化逻辑
7. 接下来就是准备训练模型了，在tinyml目录下，cd进入train目录， 执行 train_float_model_mfcc.py 训练模型， 得到训练好的模型 float_model_mfcc.tflite(fp32)以及best_float_model_mfcc.keras(fp32)
8. 对训练好的模型进行量化，执行 quantize_model_int8.py, 注意: 使用验证集的数据，选取正负样本各200条，进行归一化后的特征量化保存的best_float_model_mfcc.keras模型。权重和输入输出节点都进行了int8量化，得到量化后的模型 quantized_model_mfcc_int8.tflite
9. 测试量化模型，执行 evaluate_quantized_model.py, 对整个测试集进行测试，得到指标参数，打印指标在控制台查看损失。需要保持在可接受的范围内。注意：测试比较的模型是quantized_model_mfcc_int8.tflite（int8）和float_model_mfcc.tflite(fp32)， 对于int的输入还需要加上量化的参数。详情请查看源码。
10. 最后进行实际的音频流测试，执行detect_dog_barks_in_wav.py。模拟读取一个比较长的包含狗吠的音频文件，然后进行滑窗+步长预测，采用float32的模型和int8量化的模型进行预测，同时打印推理结果，利于观察模型效果并再次确认量化无误。 保存狗吠切片，观察模型的效果
11. 模型转换成cc数组，执行 int8_tflite_to_cc.py, 得到mfcc_int8_model.cc两个文件，保存量化模型字节数组，方便mcu端调用。
12. 讲模型数组cc文件拷贝到mcu端，以及归一化参数和模型量化输入参数以及模型反量化参数。因为在mcu端进行推理，mfcc接口得到的float特征，需要先使用归一化参数进行[-1,1]的归一化，再使用模型的量化参数，量化到[-128,127]， 喂入int8模型， 得到的int8结果再使用反量化参数，反量化到[-1,1]。得到的是标签为1，的概率（模型设计时决定的）


### 困惑点
需要理解的点：
1.为什么mfcc特征需要归一化？
- mfcc特征值范围一般不在int8的[-128,127]内，所以需要归一化，使得特征值范围[-1,1]。
- 模型训练时，使用[-1， 1]范围的输入，训练的模型会更稳定，且不会发生梯度或者权重爆炸
- 好比图像处理时，图像一般都在0-255之间，需要/255 归一化到[-1,1], 因为mfcc特征没有规定在哪个范围，所以使用脚本遍历数据集，找到高位和低位值进行归一化，使得特征值范围[-1,1]。详情查看源码
- 这是第一为了训练稳定性，第二也是为了int8的量化准备。


2.为什么量化后的模型还需要把特征值进行量化输入和反量化输出？
- 训练好的模型是float32的，在mcu端进行推理时，需要把特征值进行量化输入[-128,127]，再把结果进行反量化输出[-128,127] -> float32，才能得到正确的结果。
- tensorflow-lite设置inference_input_type = tf.int8 和 inference_output_type = tf.int8 时，整个模型（包括输入/输出）都被视为 int8 张量。， TFLite 不会自动做输入输出的 float ↔ int8 转换！这个转换必须由用户在模型外部手动完成。

3.容易让人困惑的点：
问题点：我都已经把 MFCC 归一化到 [-1,1] 了，不就是 int8 范围了吗？还量化干什么
解释点：虽然[-1,1] ≈ [-128,127]， 但是：[-1,1] 是浮点数区间，不是 int8 的区间！！如果强制转换为int8，那么就是信息压缩，而不是有效的缩放了。要知道：归一化的 [-1,1] 是浮点空间， int8 是 256 个离散整数、 两者完全不同、不能混用


结论：归一化不能替代量化。量化是为了兼容 int8 模型；归一化是为了训练稳定性。两者必须同时存在。



### 所以在mcu端的方法论是：
1. 归一化 float → [-1,1]
2. 使用 input_scale + zero_point 量化为 int8
3. 推理使用 output_scale + zero_point 反量化为 float32